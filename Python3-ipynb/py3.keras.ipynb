{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras        # 新的keras已被整合进TF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回归问题-散点图线性回归\n",
    "\n",
    "import numpy as np \n",
    "np.random.seed(1337)        # 为重现性，设置随机种子\n",
    "from tensorflow.keras.models import Sequential # 按顺序方式搭建模型\n",
    "from tensorflow.keras import Input      # 最新版本需要用单独的Input模块\n",
    "from tensorflow.keras.layers import Dense      # 全连接层\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# 构造数据\n",
    "X = np.linspace(-1,1,200)\n",
    "np.random.shuffle(X)        # 随机化处理数据\n",
    "Y = 0.5*X + 2 + np.random.normal(0,0.05,(200,)) # 内部的元组？？\n",
    "# plt.scatter(X,Y)\n",
    "# 数据集分割\n",
    "X_train, Y_train = X[:160], Y[:160] # 0-160个数据\n",
    "X_test, Y_test = X[160:], Y[160:]   # 160-200个数据\n",
    "\n",
    "# 模型搭建\n",
    "# 定义神经网络每一层            ####### 和莫烦教程有较大变化\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(1,)))        # 最新的Keras不是单独用Dense定义层，有单独的Input\n",
    "                                    # Dense定义也发生变化，需要定义输出unit\n",
    "model.add(Dense(1, activation=None))   # 定义输出维数unit即可，激活函数可不选\n",
    "# 选择损失函数和优化算法\n",
    "model.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "# 训练过程\n",
    "print('Training ...')\n",
    "for step in range(301):\n",
    "    cost = model.train_on_batch(X_train, Y_train)   # 用train_on_batch模块来训练数据\n",
    "    if step % 100 == 0:\n",
    "        print('train cost:', cost)                  # 每100次迭代输出一次信息\n",
    "\n",
    "# 测试\n",
    "print('\\nTesting ...')\n",
    "cost = model.evaluate(X_test, Y_test, batch_size=40)# evaluate模块用测试数据评估\n",
    "print('test cost:', cost)\n",
    "W,b = model.layers[0].get_weights()                 # 输出模型参数\n",
    "print('weight=',W,'\\nbias=',b)\n",
    "\n",
    "# 可视化展示训练结果\n",
    "Y_pred = model.predict(X_test)  # 计算预测值\n",
    "plt.scatter(X_test,Y_test)      # 真实值\n",
    "plt.plot(X_test,Y_pred)         # 预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类问题-MNIST手写数字数据集\n",
    "\n",
    "import numpy as np \n",
    "np.random.seed(1337)\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input          # 最新版本需要用单独的Input模块\n",
    "from tensorflow.keras.layers import Dense,Activation\n",
    "from tensorflow.keras.optimizers import RMSprop    # 优化器\n",
    "\n",
    "# MNIST数据集下载和数据集分割\n",
    "# X shape(60000, 28*28), y shape(10000)\n",
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()\n",
    "# 数据预处理-标准化、分类标签\n",
    "X_train = X_train.reshape(X_train.shape[0],-1)/255  # 像素/255颜色归一化理解\n",
    "                                # -1的解释，此函数来自numpy，具有ndarray维度推理的能力\n",
    "                                # 此处参数应是正数，-1是错误的，意为根据另一维度来推导此维度\n",
    "X_test = X_test.reshape(X_test.shape[0],-1)/255\n",
    "y_train = np_utils.to_categorical(y_train, num_classes=10)  # 分类标签，处理为'one-hot'\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# 模型搭建\n",
    "# 全连接层\n",
    "model = Sequential([    # 直接在Sequential内部搭建，用[]作为数组传入\n",
    "    Input(784),         # 输入维度时28*28=784（像素数）\n",
    "    Dense(32),\n",
    "    Activation('relu'),\n",
    "    Dense(10),\n",
    "    Activation('softmax')\n",
    "])\n",
    "# 优化器/优化算法\n",
    "rmsprop = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "# 嵌入优化器\n",
    "model.compile(\n",
    "    optimizer=rmsprop,      # 优化器是上面定义的rmsprop\n",
    "    loss='categorical_crossentropy',    # 损失函数是分类交叉熵\n",
    "    metrics=['accuracy'],   # 优化过程中同时计算准确率信息\n",
    ")\n",
    "\n",
    "# 训练过程\n",
    "print('Training ...')\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=2)    # epochs和老版本不同\n",
    "\n",
    "# 测试\n",
    "print('Testing ...')\n",
    "loss,accuracy = model.evaluate(X_test, y_test)\n",
    "print('test loss:',loss)\n",
    "print('test accuracy:',accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络模型搭建实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 卷积神经网络\n",
    "\n",
    "import numpy as np \n",
    "np.random.seed(1337)\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.models import Input          # 最新版本需要用单独的Input模块\n",
    "from keras.layers import Dense,Activation,Convolution2D,MaxPooling2D,Flatten\n",
    "from keras.optimizers import Adam       # 优化器\n",
    "\n",
    "# MNIST数据集下载\n",
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1,1,28,28)\n",
    "X_test = X_test.reshape(-1,1,28,28)\n",
    "y_train = np_utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# 模型搭建\n",
    "# 卷积层1\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(\n",
    "    filters=32,         # 过滤器，输出的大小\n",
    "    kernel_size=5,      # 卷积核大小\n",
    "    strides=1,\n",
    "    padding='same',     # 结果大小相同，即卷积前先边缘填充一圈0\n",
    "    input_shape=(1,\n",
    "                28,28),\n",
    "))\n",
    "# 激活函数\n",
    "model.add(Activation('relu'))\n",
    "# 最大池化层\n",
    "model.add(MaxPooling2D(\n",
    "    pool_size=(2,2),\n",
    "    strides=(2,2),\n",
    "    padding='same',\n",
    "))\n",
    "# 卷积层2\n",
    "model.add(Convolution2D(64,5,1,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(\n",
    "    pool_size=(2,2),\n",
    "    padding='same',\n",
    "))\n",
    "# 全连接层1\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "# 全连接层2\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# 优化器/优化算法\n",
    "adam = Adam(learning_rate=1e-4)\n",
    "# 嵌入优化器\n",
    "model.compile(\n",
    "    optimizer=adam,      # 优化器是上面定义的rmsprop\n",
    "    loss='categorical_crossentropy',    # 损失函数是分类交叉熵\n",
    "    metrics=['accuracy'],   # 优化过程中同时计算准确率信息\n",
    ")\n",
    "\n",
    "# 训练过程\n",
    "print('\\nTraining ...')\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=1)    # epochs和老版本不同\n",
    "\n",
    "# 测试\n",
    "print('\\nTesting ...')\n",
    "loss,accuracy = model.evaluate(X_test, y_test)\n",
    "print('\\ntest loss:',loss)\n",
    "print('\\ntest accuracy:',accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更紧凑的神经网络搭建方式\n",
    "\n",
    "from keras.models import Sequential\n",
    "# 模型搭建\n",
    "model = Input(shape=(28,28,3))  # 定义输入层和维度\n",
    "\n",
    "func1 = Convolution2D(filters=32, kernel_size=(1,1), strides=1, padding='same', activation='relu',)     # 过滤器，卷积核，移动步长，边缘填充，激活函数\n",
    "func2 = MaxPooling2D(pool_size=(2,2), strides=1, padding='same',)\n",
    "output = keras.layers.concatenate([func1, func2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型可视化\n",
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')\n",
    "# plot_model接收两个参数：\n",
    "    # show_shapes:      是否显示输出数据的形状，默认False\n",
    "    # show_layer_names: 是否显示层名称，默认True\n",
    "\n",
    "# 获得pydot.Graph对象，在ipython中展示：\n",
    "# pip install pydot-ng & brew install graphviz\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(model).create(prog='dot',format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras 官方英文文档"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras 开发者文档 https://keras.io/guides/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras 研究人员基础\n",
    "# https://keras.io/getting_started/intro_to_keras_for_researchers/\n",
    "\n",
    "# 导包\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras    # 作为tf的子包使用\n",
    "\n",
    "# Tensor 张量\n",
    "x = tf.constant([[5,2],[1,3]])  # 常量 张量\n",
    "# print(x)            # 打印张量\n",
    "# print(x.numpy())    # 打印张量的值（转为numpy对象了）\n",
    "x = tf.ones(shape=(2,1))        # 全一 张量\n",
    "# print(x)\n",
    "x = tf.zeros(shape=(2,1))       # 全零 张量\n",
    "# print(x)\n",
    "x = tf.random.normal(shape=(2,2), mean=0.0, stddev=1.0)    # 创建正态分布随机常量张量，指定均值和标准差\n",
    "\n",
    "# Variables 变量（TF）\n",
    "int_value = tf.random.normal(shape=(2,2))\n",
    "x = tf.Variable(int_value)      # 将数组“变量”化 - 转为TF专用变量\n",
    "# print(x)\n",
    "\n",
    "# 增加和减少变量值\n",
    "new_value = tf.random.normal(shape=(2,2))   # 设定更新的变量值\n",
    "x.assign(new_value)                         # 更新x的值\n",
    "for i in range(2):                          # 两个嵌套for循环逐个更新两个维度的数据\n",
    "    for j in range(2):\n",
    "        assert x[i,j] == new_value[i,j]\n",
    "# print(x)\n",
    "added_value = tf.random.normal(shape=(2,2)) # 设定增量\n",
    "x.assign_add(added_value)                     # 增加的x的值\n",
    "for i in range(2):                          # for循环逐个更新\n",
    "    for j in range(2):\n",
    "        assert x[i,j] == new_value[i,j] + added_value[i,j]\n",
    "# print(x)\n",
    "# 减少用x.assign_sub()方法\n",
    "\n",
    "# 数学计算\n",
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "c = a + b           # 加法，直接计算\n",
    "d = tf.square(c)    # 平方\n",
    "e = tf.exp(d)       # 指数\n",
    "# print(c, d, e)\n",
    "\n",
    "# 自动计算并监视梯度变化\n",
    "a = tf.Variable(a)\n",
    "with tf.GradientTape() as tape:     # Tape 译为磁带/录音，表示记录梯度\n",
    "    c = tf.sqrt(tf.square(a) + tf.square(b))    # 对a,b进行操作，同时记录梯度（不用手动.watch）\n",
    "    dc_da = tape.gradient(c, a)                 # 提取梯度信息到变量中\n",
    "    # print(dc_da)\n",
    "\n",
    "# Keras 定义 Layer\n",
    "# class Linear(keras.layers.Layer):   # 用 keras.layer.Layer 定义一个线性方程\n",
    "#     \"\"\"y = w.x + b\"\"\"\n",
    "#     def __init__(self, units=32, input_dim=32): # 初始化参数\n",
    "#         super(Linear, self).__init__()          # 超类\n",
    "#         w_init = tf.random_normal_initializer() # 随机初始化 w\n",
    "#         self.w = tf.Variable(                   # w 方法，生成输入的同维 w 矩阵，可训练（参数）\n",
    "#             initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "#             trainable=True,\n",
    "#         )\n",
    "#         b_init = tf.zeros_initializer()         # 初始化 b\n",
    "#         self.b = tf.Variable(\n",
    "#             initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "#         )\n",
    "#     def call(self, inputs):     # 定义线性方程\n",
    "#         return tf.matmul(inputs, self.w) + self.b\n",
    "# 调用线性方程\n",
    "# linear_layer = Linear(units=4, input_dim=2)     # 初始化线性方程对象（定义的是类）\n",
    "# y = linear_layer(tf.ones((2, 2)))   # 用全一矩阵调用方程\n",
    "# assert y.shape == (2, 4)            # assert 用于断言，此例未指定输出，输出为布尔值\n",
    "\n",
    "# 基于上例，创建该层参数的权重\n",
    "# class Linear(keras.layers.Layer):\n",
    "#     \"\"\"y = w.x + b\"\"\"\n",
    "#     def __init__(self, units=32):\n",
    "#         super(Linear, self).__init__()\n",
    "#         self.units = units\n",
    "#     def build(self, input_shape):           # 将w,b参数包进build方法中\n",
    "#         self.w = self.add_weight(           # .add_weight 为 w 参数增加权重\n",
    "#             shape=(input_shape[-1], self.units),\n",
    "#             initializer=\"random_normal\",\n",
    "#             trainable=True,\n",
    "#         )\n",
    "#         self.b = self.add_weight(           # 同样计算 b 的权重\n",
    "#             shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "#         )\n",
    "#     def call(self, inputs):\n",
    "#         return tf.matmul(inputs, self.w) + self.b\n",
    "# linear_layer = Linear(4)                    # 实例化为对象\n",
    "# y = linear_layer(tf.ones((2, 2)))           # 调用线性方程时会自动调用该方法计算权重\n",
    "\n",
    "# 层的自动求梯度\n",
    "\n",
    "# 包含层的层\n",
    "\n",
    "# 跟踪所有层的损失\n",
    "\n",
    "# 跟踪训练指标\n",
    "\n",
    "# 编译函数\n",
    "\n",
    "# 训练模型和推理模式\n",
    "\n",
    "# 功能性API快速搭建模型\n",
    "inputs = tf.keras.Input(shape=(16,), dtype=\"float32\")\n",
    "x = Linear(32)(inputs)      # 重用本页之前定义的线性方程\n",
    "x = Dropout(0.5)(x)         # 重用本页之前定义的\n",
    "outputs = Linear(10)(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "assert len(model.weights) == 4\n",
    "y = model(tf.ones((2, 16)))\n",
    "assert y.shape == (2, 10)\n",
    "y = model(tf.ones((2, 16)), training=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 自动编码器 - 处理MNIST手写数字识别 - Keras API版本\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "# Define encoder model.\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,), name=\"encoder_input\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name=\"encoder\")\n",
    "\n",
    "# Define decoder model.\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name=\"z_sampling\")\n",
    "x = layers.Dense(intermediate_dim, activation=\"relu\")(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation=\"sigmoid\")(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name=\"decoder\")\n",
    "\n",
    "# Define VAE model.\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name=\"vae\")\n",
    "\n",
    "# Add KL divergence regularization loss.\n",
    "kl_loss = -0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "vae.add_loss(kl_loss)\n",
    "\n",
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    ")\n",
    "dataset = dataset.map(lambda x: (x, x))  # Use x_train as both inputs & targets\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "# Configure the model for training.\n",
    "vae.compile(optimizer, loss=loss_fn)\n",
    "\n",
    "# Actually training the model.\n",
    "vae.fit(dataset, epochs=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras 应用程序接口 https://keras.io/api/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras 代码示例 https://keras.io/examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
